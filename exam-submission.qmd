---
title: "ETX5551 Exam S1 2025"
author: "YOUR NAME"
date: "2025-06-27"
quarto-required: ">=1.6.0"
format:
    unilur-html+solution:
        output-file: exam-submission.html
        css: "assignment.css"
        embed-resources: true
unilur-solution: true
---

**As per [Monash's integrity rules](https://www.monash.edu/student-academic-success/learnhq/maintain-academic-integrity) this assignment needs to be completed independently and not shared beyond this class.**


## ðŸ”‘ Instructions

- This is an open book exam, and you are allowed to use any resources that you find helpful, including Generative AI. 
- Write your answers into the solutions part of the `exam-solution.qmd` file provided, render and upload to your GitHub repo when finished.

## Exercises

```{r}
#| echo: false
#| message: false
library(tidyverse)
library(tourr)
library(mulgar)
library(geozoo)
library(GGally)
library(ggdendro)
library(randomForest)
library(classifly)
library(DT)
```


### 1.  Warm-up (5pts)

The simulated data in `c5551.rda` has 5 variables. What is its shape? Solid or hollow, sphere, cube, torus, hexagonal prism, ellipsoid, roman surface, or mobius strip? Explain your reasoning.

::: unilur-solution

```{r}
#| code-fold: true 
load("data/c5551.rda")
```

YOUR ANSWER HERE

:::

### 2. Dimension reduction (25pts)

The data `feats_all.rda` has a collection of data on 200 time series, common macroeconomic and microeconomic series, extracted from [A self-organizing, living library of time-series data](https://www.comp-engine.org). Each of the series has been converted to time series features, using the time series features available in the [feasts](https://feasts.tidyverts.org) package. These include measures of the trend, seasonality, autocorrelation, jumps and variance. There are 37 variables, all of which are features.


##### a. (5pts) Using a grand tour on the full set of 37 variables, describe the structure of this data (e.g. outliers, clustering, linear association, non-linear association). *Ignore the variable named `type` for this exercise.*

::: unilur-solution

```{r}
#| code-fold: true
load("data/feats_all.rda")
```

YOUR ANSWER HERE

:::

##### b. The plots (@fig-scree, @fig-scatmat) and table (@tbl-coefs) below summarise the principal component analysis of the data. The data containing the first five principal components is available in the `feats_pc_d.rda` dataset. *Ignore the variables named `type`, `cl2`, `cl3`, `cl4`, `cl5`, `cl6` for this exercise.*

- (3pts) Explain why two principal components is not enough to summarise the variability in this data.
- (2pts) Why would five principal components be a good choice?
- (5pts) Using a grand tour to examine the first five PCs. Describe the structure that is still present in the data when it is reduced from 37 variables to 5 (clustering, outliers, nonlinear association).
- (5pts) There is an outlier in PC4. On which of the time series features (`trend_strength`, ..., `stat_arch_lm`) does this time series have high values? So how would the time series of this point appear (strong trend, seasonality, peaks, spikiness, ...) ? 

```{r}
#| echo: false
#| label: fig-scree
#| fig-cap: "Scree plot of the principal component analysis of the economic time series features data."
feats_pc <- prcomp(feats_all[,1:37], scale=TRUE)
ggscree(feats_pc, q=37)
```

```{r}
#| echo: false
#| label: tbl-coefs
#| tbl-cap: "Coefficients of the first five principal components."
feats_coef <- feats_pc$rotation[,1:5]
feats_pc_d <- feats_pc$x[,1:5] |>
  as_tibble() |>
  mutate(type = feats_all$type)

datatable(feats_coef, 
          options = list(pageLength = 40)) |> 
  formatRound(columns=c('PC1', 'PC2', 'PC3', 
                        'PC4', 'PC5'), digits=3)
```

```{r}
#| echo: false
#| label: fig-scatmat
#| fig-cap: "Scatterplot matrix of the first five principal components of the economic time series features data."
ggscatmat(feats_pc_d[,1:5])
```

```{r}
#| echo: false
# Clustering results
feats_hc <- hclust(dist(feats_pc_d[,1:5]), method = "ward.D2")

feats_pc_d <- feats_pc_d |>
  mutate(
    cl2 = factor(cutree(feats_hc, 2)),
    cl3 = factor(cutree(feats_hc, 3)),
    cl4 = factor(cutree(feats_hc, 4)),
    cl5 = factor(cutree(feats_hc, 5)),
    cl6 = factor(cutree(feats_hc, 6)))

save(feats_pc_d, file="data/feats_pc_d.rda")
```

::: unilur-solution

YOUR ANSWER HERE

:::

##### c. (5pts) If you were to make a 5D model and overlay it on the data in 5D, how well do you anticipate it fits? Good fit, poor fit, with reasons.

::: unilur-solution

YOUR ANSWER HERE

:::

### 3. (25pts) Clustering

This question uses the time series features data also. Below is the dendrogram of hierarchical clustering conducted on the first five principal components. 

```{r}
#| echo: false
#| label: fig-dendro
#| fig-cap: "Dendrogram summarising the hierarchical clustering of the first five principal components of the time series features data."
ggdendrogram(feats_hc)
```

##### a. (3pts) Based on the dendrogram, how many clusters would you suggest are reasonable to consider? Explain your answer.

::: unilur-solution

YOUR ANSWER HERE

:::

##### b. (4pts) Cross-tabulate the two cluster solution `cl2` and the original type of series variable `type`. Using this table, and grand tour of the first five PCs, coloured by each of these two results, describe how they are similar or not.

::: unilur-solution

YOUR ANSWER HERE

:::

##### c. (8pts) Using the grand tour, and possibly a guided tour, come to a decision about which number of clusters (2, 3, 4, 5, or 6) is the best for this data.
::: unilur-solution

YOUR ANSWER HERE

:::

##### d. (5pts) Compare your best result with the original series `type`. Why would it be ideal for the final clustering to create clusters that were primarily one or other type? That is, the `macro` series are sub-divided into multiple clusters, but they mostly contain only `macro` series, and similarly `micro` series are broken into multiple clusters mostly only containing other `micro` series. Does your best result do this, or not? And if not, why is it still a reasonable result?

::: unilur-solution

YOUR ANSWER HERE

:::

##### e. (5pts) Clustering was done on the first five principal components. Justify that this was a good choice for this data, given the structure that you described in 2a from the initial visualisation of the full 37-dimensional space?

::: unilur-solution

YOUR ANSWER HERE

:::

### 4. (5pts) For your best clustering result 

- Remove any clusters that contain only one observation.
- Use the random forest algorithm to build a model to predict the class. 

Then answer either A or B

A. Use the `explore` function of the `classifly` package to predict a full 5D cube of points, in order to examine the boundary or partitioning that the clustering has imposed on the data. With this display, and the summary of variable importance, explain which principal components contribute to the difference/distinction between clusters. 

B. Examine the votes matrix as a simplex. Along with the confusion matrix, describe which clusters are most likely confused with each other.

Make sure to include a picture to support your arguments.

::: unilur-solution

YOUR ANSWER HERE

:::